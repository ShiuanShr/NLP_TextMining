{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"datacleaning_content1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 簡述\n"," \n","**`這份檔案下方有利用transfer learning 的GPT2，可用於作資料的summarization，但是基於我們資料量太少，該檔案只用於整理文章內文content資料，用意在於將整理好的content存到Clean_Content資料夾，如此，Final_Code/BERT資料夾內的Bert_transfer_learning_Leo可以使用乾淨的content，並將title與contentconcate並且截斷成120字，餵給BERT`**\n","\n","\n","**- import ducuments path:**\n","\n","/content/drive/Shareddrives/Text Mining/CSV/Crawler/\n","\n","\n","\n","\n","**- output ducuments path1:** \n","\n","/content/drive/Shareddrives/Text Mining/CSV/Clean_Content/\n","\n","\n","**- output ducuments path2: **\n","\n","/content/drive/Shareddrives/Text Mining/CSV/Summary/Stage1/\n","\n","此資料夾放合併的data，但是尚未加入BERT預測結果\n","\n","\n","\n","\n","\n"],"metadata":{"id":"qAS4rJ1Si-QG"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzFAW8nJHx18","outputId":"7fa8d58a-7447-4a92-c6c5-7038e052a6be","executionInfo":{"status":"ok","timestamp":1654587601028,"user_tz":-480,"elapsed":25331,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SqSB2uGlVBJn","outputId":"3540cb6a-9b51-443a-9f93-92e2203d3a23","executionInfo":{"status":"ok","timestamp":1654587605361,"user_tz":-480,"elapsed":4336,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ckiptagger\n","  Downloading ckiptagger-0.2.1-py3-none-any.whl (34 kB)\n","Installing collected packages: ckiptagger\n","Successfully installed ckiptagger-0.2.1\n"]}],"source":["# 安裝套件\n","\n","!pip install -U ckiptagger\n","\n","# 下載資料包 法1\n","# from ckiptagger import data_utils\n","# data_utils.download_data_gdown(\"./\")  #系統可能會當 因為檔案太大\n","\n","# 下載資料包 法2 穩定\n","import gdown\n","\n","url = \"https://drive.google.com/uc?id=1efHsY16pxK0lBD2gYCgCTnv1Swstq771\"\n","#output = \"data.zip\"\n","# quit = False\n","\n","# output = \"/content/drive/MyDrive/NTHU/TextMining/Text Mining Team Project/data\"\n","# gdown.download(url,output,quit)\n","# #(來源,目的, 顯示下載進度與否)\n","import re\n","# # 解壓縮\n","# !unzip data.zip\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"code","source":["# 方法3\n","! pip3 install ckiptagger\n","! pip3 install tensorflow\n","! pip3 install gdown\n","! pip install --upgrade --no-cache-dir gdown\n","# -*- coding: utf-8 -*-\n","from ckiptagger import data_utils\n","data_utils.download_data_gdown(\"./\")\n","# -*- coding: utf-8 -*-\n","from ckiptagger import WS, POS, NER\n","\n","# text = '傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。'\n","ws = WS(\"./data\")\n","pos = POS(\"./data\")\n","ner = NER(\"./data\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dTZsZ_HXVUbQ","outputId":"481b48eb-5b2b-442c-b2b0-4deaf0117a22","executionInfo":{"status":"ok","timestamp":1654587680827,"user_tz":-480,"elapsed":75470,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ckiptagger in /usr/local/lib/python3.7/dist-packages (0.2.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220527125636)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.3)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n","Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.5.18.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.7.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.5.18.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"]},{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1efHsY16pxK0lBD2gYCgCTnv1Swstq771\n","To: /content/data.zip\n","100%|██████████| 1.88G/1.88G [00:10<00:00, 179MB/s]\n","/usr/local/lib/python3.7/dist-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n","  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n","/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n","  partitioner=maybe_partitioner)\n","/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n","  initializer=initializer)\n","/usr/local/lib/python3.7/dist-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n","  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n","/usr/local/lib/python3.7/dist-packages/ckiptagger/model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n","  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"]}]},{"cell_type":"code","source":["# 下載台北思源黑體，並命名taipei_sans_tc_beta.ttf\n","!wget -O taipei_sans_tc_beta.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n","!pip install wordcloud\n","!pip install zhon\n","from zhon.hanzi import punctuation\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from string import punctuation"],"metadata":{"id":"hPNcTiK1VsDY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654587694831,"user_tz":-480,"elapsed":14009,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}},"outputId":"6d8d1b93-cd53-4835-e2fb-c33c34098303"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-06-07 07:41:19--  https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n","Resolving drive.google.com (drive.google.com)... 142.250.101.100, 142.250.101.138, 142.250.101.101, ...\n","Connecting to drive.google.com (drive.google.com)|142.250.101.100|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ppqhtp89bsltmaof1lm005qru46iu9u0/1654587675000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_ [following]\n","Warning: wildcards not supported in HTTP.\n","--2022-06-07 07:41:20--  https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ppqhtp89bsltmaof1lm005qru46iu9u0/1654587675000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n","Resolving doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)... 142.250.141.132, 2607:f8b0:4023:c0b::84\n","Connecting to doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 20659344 (20M) [application/x-font-ttf]\n","Saving to: ‘taipei_sans_tc_beta.ttf’\n","\n","taipei_sans_tc_beta 100%[===================>]  19.70M  --.-KB/s    in 0.07s   \n","\n","2022-06-07 07:41:21 (269 MB/s) - ‘taipei_sans_tc_beta.ttf’ saved [20659344/20659344]\n","\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.21.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting zhon\n","  Downloading zhon-1.1.5.tar.gz (99 kB)\n","\u001b[K     |████████████████████████████████| 99 kB 4.5 MB/s \n","\u001b[?25hBuilding wheels for collected packages: zhon\n","  Building wheel for zhon (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for zhon: filename=zhon-1.1.5-py3-none-any.whl size=84322 sha256=d74b18ee1c8e3468ae094034207c6f9b22d710a8d36127e05b247d9b53d76014\n","  Stored in directory: /root/.cache/pip/wheels/d0/56/17/2675c4c7413a72bf173062e8d0a16503e3b2607745aa84988d\n","Successfully built zhon\n","Installing collected packages: zhon\n","Successfully installed zhon-1.1.5\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import_pwd = '/content/drive/Shareddrives/Text Mining/CSV/Crawler/'\n","\n","\n","# GUO\n","name = \"content_GUO.csv\"\n","input_GUO = pd.read_csv(import_pwd +'GUO_content_output.csv')\n","text_list = input_GUO.columns\n","text_list[:4]\n"],"metadata":{"id":"QIa46oN_VpVJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654587697052,"user_tz":-480,"elapsed":2223,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}},"outputId":"47330e0c-a3f7-41f3-ab2f-06549b17d124"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['\\n郭婞淳又再次拿到世錦賽金牌了\\n\\n舉重算是世界熱門項目，\\n\\n中國跟歐美體育強國為什麼無法在同量級培養能超越台灣郭婞淳的選手？\\n\\n\\n',\n","       '\\n備註請放最後面 違者新聞文章刪除\\n\\n1.媒體來源:\\n※ 例如蘋果日報、自由時報（請參考版規下方的核准媒體名單）\\n\\n自由時報\\n2.記者署名:\\n※ 若新聞沒有記者名字或編輯名字，請勿張貼，否則會被水桶14天\\n※ 外電至少要有來源或編輯 如:法新社\\n記者林岳甫／綜合報導\\n\\n3.完整新聞標題:\\n※ 標題沒有完整寫出來 -> 依照板規刪除文章\\n世錦賽》「舉重女神」郭婞淳挺舉、總和金牌入袋 生涯10金創紀錄\\n\\n4.完整新聞內文:\\n※ 社論特稿都不能貼！違者刪除（政治類水桶3個月)，貼廣告也會被刪除喔！可詳看版規\\n\\n郭婞淳挺舉130公斤、總和230公斤本屆抱回2金、1銀。（資料照，特派記者林正堃攝）\\n\\n〔記者林岳甫／綜合報導〕「舉重女神」郭婞淳今日在2021年世界錦標賽女子59公斤級，\\n抓舉100公斤拿銀牌，再來挺舉130公斤、總和230公斤技壓群雌，本屆抱回2金、1銀，迄\\n今她在世錦賽連4屆收下總和金牌，增添明年拚亞運2連霸的信心。\\n\\n郭婞淳2017年奪58公斤級總和金牌，但2018年更改量級後，郭婞淳同年及2019年順利拿到\\n59公斤級總和金牌，本屆將尋求女子59公斤級總和3連霸。\\n東京奧運奪金的郭婞淳，雖保有抓舉110公斤、挺舉140公斤、總和247公斤的3項世界紀錄\\n，但本屆世錦賽以不受傷為前提，同時也要透過大賽瞭解各國好手的狀況。\\n\\n郭婞淳抓舉開把重量原為95公斤，她把重量加到97公斤，登場後一鼓作氣舉起，第二把升\\n至100公斤快意舉起，最後一把增加至102公斤失敗，以100公斤不敵21歲烏克蘭選手漢柯\\n爾（Mariia Hanhur）的101公斤，抓舉收下銀牌。\\n\\n即使沒辦法先拿金牌，郭婞淳進到最擅長挺舉項目就有機會討回顏面，開把125公斤技壓\\n群雌，她上場後順利舉起，且哥倫比亞選手艾瓦瑞茲最後一把挑戰127公斤成功，但郭婞\\n淳在第二把舉起128公斤，提前篤定挺舉、總和金牌到手，她最後一把挑戰130公斤又成功\\n。\\n\\n郭婞淳從2013年開啟征戰世錦賽旅程，本屆收下2金、1銀，生涯累積10金、5銀、2銅，超\\n越陳瑞蓮的9金紀錄，成為台灣在世錦賽最多金女將。\\n\\n5.完整新聞連結 (或短網址):\\n※ 當新聞連結過長時，需提供短網址方便網友點擊\\nhttps://sports.ltn.com.tw/news/breakingnews/3765767\\n\\n6.備註:\\n※ 一個人一天只能張貼一則新聞(以天為單位)，被刪或自刪也算額度內，超貼者水桶，請注意\\n※ 備註請勿張貼三日內新聞(包含連結、標題等)\\n\\n\\n真台灣之光\\n但是有新聞\\n說2028年會取消舉重的奧運比賽\\n\\n希望別取消\\n\\n讓她可在為台灣舉出金牌\\n\\n\\n\\n\\n',\n","       '\\n2020東京奧運舉重金牌得主郭婞淳\\n\\n於2021年世界舉重錦標賽的女子59公斤級\\n\\n以抓舉100公斤、挺舉130公斤，總和230公斤\\n\\n抓舉銀牌、挺舉&總和獲得金牌!!!\\n\\n個人累積世錦賽10金\\n\\n真是太厲害啦!!!\\n\\n恭喜～～\\n\\n\\n',\n","       '\\n如題\\n\\n現在體育選手廣告很多\\n\\n郭婞淳有個廣告講比較多話\\n\\n國語超不標準耶\\n\\n一聽就南部上來的\\n\\n拍廣告至少把國語練好吧\\n\\n人夠壯了 國語再不標準\\n\\n令人擔心以後的感情路\\n\\n有八卦嗎\\n\\n\\nSent from nPTT on my iPhone 11\\n\\n\\n'],\n","      dtype='object')"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## 進行停用字設定\n","\n","Rule Base:\n","\n","資料清理: \n","\n","- 移除英文、標點、空白 \n","- 避免中英混雜 (將element encode然後判斷是不是英文) \n","- 自建停用字\n","- 英文半形標點符號\n","- 中文全形標點符號\n","- 破壞複合符號組成之顏文字\n","\n","(可加強處:\n","- 希臘字文字取代 (特殊顏文字應用)特殊顏文字應用) \n","- 保留表情符號，並將其映射成中文字，例如 👍 >> 讚"],"metadata":{"id":"z_Mc1V2IVbnI"}},{"cell_type":"code","source":["from string import punctuation\n","from wordcloud import STOPWORDS\n","import numpy as np \n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","%config InlineBackend. figure_format = 'retina'\n","# 強迫提升matplotlib圖形顯示度"],"metadata":{"id":"Vut7Kh1MVUgA","executionInfo":{"status":"ok","timestamp":1654587697052,"user_tz":-480,"elapsed":6,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["## 自建停用字\n","my_stop_words = ['：','、','※','【','】','※', '」'] # '，', '。'\n","term_stopwords = ['什麼','可以','挑選','篇文章', '備註', '是', '新聞網','或', '的', '某' ,'啦', '分', '等', '才能','可以','我們','挑選','篇文章','你',\n","        '知道','任何','如果','而且','不會','。',\n","        '從來','外有','不論','從來','還是','这是','以便',\n","        '放在','問個','同時','需登','有些','不用','一定','什麼',\n","        '運動給','这次','你','我','他','']\n","special_sign = ['◆','▆','▍','▋','▲','◤','▼','▲','▲','◣','▎','◢', '～', '○', '●', '▼', '▲', '->', ':',\n","                '.','~~','!', '.', '/', '\\ ']\n","full_stopwords = term_stopwords + special_sign + my_stop_words #+list(STOPWORDS)\n","punctuation_str = punctuation\n","punctuation_str"],"metadata":{"id":"Mm-6REi-VUin","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1654587697053,"user_tz":-480,"elapsed":7,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}},"outputId":"4c1548d6-18c1-463c-ca27-80579697e595"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["def token_pack(text):\n","  ws_results = ws([text])\n","  pos_results = pos(ws_results)\n","  ner_results = ner(ws_results, pos_results)\n","  # print('\\nws result: \\n')\n","  # print(ws_results)\n","  # print('\\nPOS result: \\n')\n","  # print(pos_results)\n","  # for name in ner_results[0]:\n","  #   print(name)\n","  # 這邊我們直接丟掉NER與POS的結果\n","  text = ws_results\n","  return text \n","\n","def data_cleaning(str_list):\n","  string_content = str_list\n","  article_1 = [article.replace('\\n', '') for article in string_content]\n","  tokens_list = [ token_pack(article) for article in article_1]\n","  clean_sentences = []\n","  for article in tokens_list:\n","    clean_tokens = []\n","    for _ in article:\n","      for token in _:\n","        # 去除token所有半形空白\n","        token = token.replace(' ', '')\n","        # 前後去除空白 #去除中英夾雜 #去除純數字\n","        if token.strip() != \"\" and not token.encode().isalpha()  and token.isdigit() == False:\n","          # 去除自建停用字\n","          if not token in full_stopwords:\n","            # 去除英文半形標點符號  #去除指定中文之標點符號\n","            if  not token in punctuation and not token in punctuation_str:\n","              clean_tokens.append(token)\n","      if len(clean_tokens)>0:\n","        clean_sentences.append(clean_tokens)\n","  return clean_sentences"],"metadata":{"id":"nwBE6G_pVUlF","executionInfo":{"status":"ok","timestamp":1654587697053,"user_tz":-480,"elapsed":5,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def clean(string_content):\n","  article_1 = [article.replace('\\n', '').\n","             replace('若新聞沒有記者名字或編輯名字', '').\n","             replace('備註請放最後面 違者新聞文章刪除','').\n","             replace('媒體來源', '').\n","             replace('標題沒有完整寫出來','').\n","             replace('完整新聞標題','').\n","             replace('完依照板規刪除文章','').\n","             replace('記者署名', '').\n","             replace('請勿張貼，否則會被水桶14天', '').\n","             replace('請參考版規下方的核准媒體名單', '').\n","             replace('綜合報導', '').\n","             replace('記者', '').\n","             replace('超貼者水桶者', '').\n","             replace('被刪或自刪也算額度內', '').\n","             replace('新聞連結過長時需提供短網址方便網友點擊', '').\n","             replace('新聞連結過長時', '').\n","             replace('以天為單位', '').\n","             replace('請勿張貼三日內新聞(包含連結、標題等)', '').\n","             replace('蘋果日報', '').\n","             replace('蘋果', '').\n","             replace('直播', '').\n","             replace('連結', '').\n","             replace('外電', '').\n","             replace('編輯', '').\n","             replace('自由時報', '').\n","             replace('聯合報', '').\n","             replace('依照板規刪除文章', '').\n","             replace('政治類', '').\n","             replace('可詳看版規', '').\n","             replace('水桶', '').\n","             replace('違者刪除', '').\n","             replace('超貼者', '').\n","             replace('個月', '').\n","             replace('超貼者', '').\n","             replace('超貼者', '').\n","             replace('超貼者', '').\n","             replace('請注意', '').\n","             replace('的', '').\n","             replace('廣告也會被刪除', '').\n","             replace('一個人一天只能張貼一則', '')\n","             for article in string_content]\n","#  [、_ =\\ \\\\ [\\ ] ; ' ，. / ~ ! @ # $ % ^ & *  + | ? > < 「 :{}]\n","\n","  final_list = [re.sub('[^\\u4E00-\\u9FA5 + ，! @ # ? & ? > < 「」 {}]', '', i) for i in article_1]\n","  final_list = [re.sub(' ', '', i) for i in final_list]\n","\n","  return final_list\n","\n"],"metadata":{"id":"VJx6Fo3DXbSR","executionInfo":{"status":"ok","timestamp":1654587697053,"user_tz":-480,"elapsed":5,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Save the cleaned content to Clean_content folder"],"metadata":{"id":"Gay_wRL1O5a5"}},{"cell_type":"code","source":["# PATH\n","# import_pwd = '/content/drive/MyDrive/NTHU/TextMining/Text Mining Team Project/CSV/'\n","# save_path = \"/content/drive/MyDrive/NTHU/TextMining/Text Mining Team Project/CSV/content_data_preprocessing/\"\n","import_pwd = '/content/drive/Shareddrives/Text Mining/CSV/Crawler/'\n","save_path = \"/content/drive/Shareddrives/Text Mining/CSV/Clean_Content/\"\n","\n","# GUO \n","name = \"content_GUO.csv\"\n","GUO = pd.read_csv(import_pwd +'GUO_content_output.csv')\n","text_list = GUO.columns\n","text_list_GUO = clean(text_list)\n","GUO_df = pd.DataFrame({'content': text_list_GUO})\n","path = save_path + name\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  GUO_df.to_csv(f)\n","\n","# LEE\n","name = \"content_LEE.csv\"\n","LEE = pd.read_csv(import_pwd +'LEE_content_output.csv')\n","text_list = LEE.columns\n","LEE_df = pd.DataFrame({'content': clean(text_list)})\n","path = save_path + name\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  LEE_df.to_csv(f)\n","\n","# LIN\n","name = \"content_LIN.csv\"\n","LIN = pd.read_csv(import_pwd +'LIN_content_output.csv')\n","text_list = LIN.columns\n","LIN_df = pd.DataFrame({'content': clean(text_list)})\n","path = save_path + name\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  LIN_df.to_csv(f)\n","\n","# TAI\n","name = \"content_TAI.csv\"\n","TAI = pd.read_csv(import_pwd +'TAI_content_output.csv')\n","text_list = TAI.columns\n","TAI_df = pd.DataFrame({'content': clean(text_list)})\n","path = save_path + name\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  TAI_df.to_csv(f)\n","\n","# YANG\n","name = \"content_YANG.csv\"\n","YANG = pd.read_csv(import_pwd +'YANG_content_output.csv')\n","text_list = YANG.columns\n","YANG_df = pd.DataFrame({'content': clean(text_list)})\n","path = save_path + name\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  YANG_df.to_csv(f)"],"metadata":{"id":"jHuKgVxVjgRG","executionInfo":{"status":"ok","timestamp":1654587701526,"user_tz":-480,"elapsed":4478,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["YANG_df['content'][4]"],"metadata":{"id":"tUGB3fztzfCL","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1654587701526,"user_tz":-480,"elapsed":6,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}},"outputId":"e8e371d6-4458-4105-a96f-01ac3fc58b35"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'阿布達比大滿貫賽楊勇緯奪生涯首金穩住世界第一寶座年月日新聞雲路皓惟今年最後一場國際賽阿布達比大滿貫賽，今日男子公斤級賽事，世界排名第一台灣「柔道男神」楊勇緯勢如破竹連過關後，在金牌戰擊退今年巴黎大滿貫銀牌得主俄羅斯好手阿卜杜拉耶夫，摘下生涯大滿貫賽首金男子公斤級共分為組，楊勇緯首輪對上阿聯酋選手納克比，納克比開賽後就先吞次指導，接著在比賽剩餘分秒吞下第次指導，楊勇緯輕鬆奪勝，順利挺進次輪次輪強楊勇緯與烏克蘭選手哈爾馬托夫交手，比賽剩餘分秒，對手先吞下一次指導，接著楊勇緯在倒數秒時，以一本贏下比賽，晉級強準決賽對上英國選手霍爾，楊勇緯使出招牌「三角固」壓制對手，最終以一本勝拿下勝利，順利挺進金牌戰金牌戰楊勇緯與年巴黎大滿貫賽銀牌得主阿卜杜拉耶夫過招，開賽雙方就採取主動攻勢，隨後對手先吞下次指導，接著楊勇緯試圖在地面上試圖壓制對手，但對手頑強抵抗，最後倒數秒，楊勇緯使出「過肩摔」摔倒對手，奪下金牌過去楊勇緯參加大滿貫賽，最佳成績為年在德國杜賽道夫柔道大滿貫拿下銀牌，另外在年大阪和年安塔利亞則是皆收穫銅牌，今年阿布達比大滿貫成功奪得生涯首金，也刷新大滿貫賽個人最佳成績楊勇緯今年在國際賽表現搶眼，在卡達大師賽摘銀安塔利亞大滿貫賽奪銅亞洲及太平洋錦標賽也拿下銀牌，東奧再進帳一面銀牌，以積分登上世界第一，成為台灣柔道史上第一人，這次在阿布達比大滿貫摘金，世界第一位置更穩固?太強啦奧運一定再金牌吧'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["def histo_wordcount(data , title , bin=25):\n","  data[\"Len\"] = data[\"content\"].map(len)\n","\n","  n, bins, patches=plt.hist(data[\"Len\"],bins=bin)\n","\n","  mean_word = round(data[\"Len\"].mean(),1)\n","  median_word = round(data[\"Len\"].median(),1)\n","  # max_hight = data[\"Len\"].max()/bin\n","\n","  plt.xlabel(\"word count\")\n","  plt.ylabel(\"sentence count\")\n","\n","  plt.vlines(mean_word,0,22,color=\"black\")\n","  plt.vlines(median_word,0,22,color=\"red\")\n","  plt.annotate(f'Mean: {mean_word}', xy=(mean_word, 15), xytext=(mean_word + 100, 18),arrowprops=dict(facecolor='black', shrink=0.05))\n","  plt.annotate(f'Median: {median_word}', xy=(median_word, 20), xytext=(median_word + 100, 22),arrowprops=dict(facecolor='red', shrink=0.05))\n","  \n","  plt.title(title+\"- WordCount Histogram\")\n","  return plt.show()\n","# plt.subplot(1, 3, 1)                 # plt.subplot(列數, 行數, 圖形編號)設定第一張圖位置\n","# histo_wordcount(YANG_df, 'YANG', 30)\n","# histo_wordcount(TAI_df, 'TAI',30)\n","# histo_wordcount(LEE_df, 'LEE',30)\n","# histo_wordcount(LIN_df, 'LIN',30)\n","# histo_wordcount(GUO_df, 'GUO',30)  "],"metadata":{"id":"WUZTdD3Bvpws","executionInfo":{"status":"ok","timestamp":1654587701526,"user_tz":-480,"elapsed":5,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## 合併title與清理後的content"],"metadata":{"id":"8JxZt9lwOeAU"}},{"cell_type":"code","source":["def title_cleaner(string_content):\n","  # 此函式用來清理title資料\n","  article_1 = [article.replace('\\n', '').\n","             replace('re', '')\n","             for article in string_content]\n","\n","  final_list = [re.sub('[^\\u4E00-\\u9FA5 + ，! @ # ? & ? > < 「」 {}]', '', i) for i in article_1]\n","  final_list = [re.sub(' ', '', i) for i in final_list]\n","\n","  return final_list"],"metadata":{"id":"ugnjCrLFQQUu","executionInfo":{"status":"ok","timestamp":1654587701527,"user_tz":-480,"elapsed":5,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def bert_food_tester(df, start, end):\n","  # 確定limited_Bert欄位沒有長度超果123的sentence\n","  for i in range(start, end+1):\n","    if end == 100:\n","      print('Out of index')\n","      break\n","    len_value = len(df['limited_Bert'][i])\n","    if i % 9 == 0:\n","      print(f'The {i} th round is completed')\n","    if i == end:\n","      print(f'its the final round ({i} round), all finish')\n","    \n","    if len_value > 123:\n","      print('Its out of range')\n","      break\n","def title_content_merger(import_pwd,title_name,import_pwd_content, clean_content_name):\n","  # title處理\n","  title_input = pd.read_csv(import_pwd + title_name)\n","  clean_title =title_cleaner(title_input.columns) # 清理title\n","  \n","  clean_title_without_label =  [ i[2:] for i in clean_title] #取出 問卦、新聞等標籤\n","  Category = [ i[:2] for i in clean_title] # 取出非 問卦、新聞的title文字\n","\n","  clean_df = pd.DataFrame({'category':Category, 'title': clean_title_without_label}) #將清理好標題寫成DataFrame\n","  clean_df.reset_index(inplace=True)\n","\n","  # content處理\n","  cleaned_content = pd.read_csv(import_pwd_content + clean_content_name)\n","  content_df = cleaned_content.set_axis(['index', 'content'], axis=1, inplace=False)\n","  content_df.set_index('index', inplace=True)\n","  # 合併\n","  merge_df = pd.merge(clean_df, content_df ,on = 'index',how='inner')\n","\n","  merge_df['limited_Bert']= merge_df['title'] + ':'+ merge_df['content']\n","  merge_df['limited_Bert'] = [ i[:122] for i in merge_df['limited_Bert']]\n","  bert_food_tester(merge_df, 0,len(merge_df)-1)\n","\n","  return merge_df\n","\n","\n"],"metadata":{"id":"gAy1XcSQObB1","executionInfo":{"status":"ok","timestamp":1654587701527,"user_tz":-480,"elapsed":5,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# GUO\n","\n","import_pwd = '/content/drive/Shareddrives/Text Mining/CSV/Crawler/'\n","title_name = 'GUO_title_output.csv'\n","import_pwd_content = '/content/drive/Shareddrives/Text Mining/CSV/Clean_Content/'\n","clean_content_name = 'content_GUO.csv'\n","\n","GUO_df = title_content_merger(import_pwd,title_name,import_pwd_content,clean_content_name)\n","# Save \n","save_path = \"/content/drive/Shareddrives/Text Mining/CSV/Summary/Stage1/\"\n","name = \"All_GUO.csv\"\n","path = save_path + name\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  GUO_df.to_csv(f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rp5gl_NCcgcr","executionInfo":{"status":"ok","timestamp":1654587703025,"user_tz":-480,"elapsed":1502,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}},"outputId":"d8bc6b99-0c9c-4b1f-a197-3609a177210b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["The 0 th round is completed\n","The 9 th round is completed\n","The 18 th round is completed\n","The 27 th round is completed\n","The 36 th round is completed\n","The 45 th round is completed\n","The 54 th round is completed\n","The 63 th round is completed\n","The 72 th round is completed\n","The 81 th round is completed\n","The 90 th round is completed\n","The 99 th round is completed\n","its the final round (99 round), all finish\n"]}]},{"cell_type":"code","source":["# LIN\n","\n","import_pwd = '/content/drive/Shareddrives/Text Mining/CSV/Crawler/'\n","title_name = 'LIN_title_output.csv'\n","import_pwd_content = '/content/drive/Shareddrives/Text Mining/CSV/Clean_Content/'\n","clean_content_name = 'content_LIN.csv'\n","\n","LIN_df = title_content_merger(import_pwd,title_name,import_pwd_content,clean_content_name)\n","\n","# Save \n","save_path = \"/content/drive/Shareddrives/Text Mining/CSV/Summary/Stage1/\"\n","name = \"All_LIN.csv\"\n","path = save_path + name\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  LIN_df.to_csv(f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0MVlsn_fm-D","executionInfo":{"status":"ok","timestamp":1654587704250,"user_tz":-480,"elapsed":1227,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}},"outputId":"ba15875e-e5ea-40a7-c6a2-b9846e15565d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["The 0 th round is completed\n","The 9 th round is completed\n","The 18 th round is completed\n","The 27 th round is completed\n","The 36 th round is completed\n","The 45 th round is completed\n","The 54 th round is completed\n","The 63 th round is completed\n","The 72 th round is completed\n","The 81 th round is completed\n","The 90 th round is completed\n","The 99 th round is completed\n","its the final round (99 round), all finish\n"]}]},{"cell_type":"code","source":["# TAI\n","\n","import_pwd = '/content/drive/Shareddrives/Text Mining/CSV/Crawler/'\n","title_name = 'TAI_title_output.csv'\n","import_pwd_content = '/content/drive/Shareddrives/Text Mining/CSV/Clean_Content/'\n","clean_content_name = 'content_TAI.csv'\n","\n","TAI_df = title_content_merger(import_pwd,title_name,import_pwd_content,clean_content_name)\n","\n","# Save \n","save_path = \"/content/drive/Shareddrives/Text Mining/CSV/Summary/Stage1/\"\n","name = \"All_TAI.csv\"\n","path = save_path + name\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  TAI_df.to_csv(f)"],"metadata":{"id":"rmgXAZh8fnSQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654587704913,"user_tz":-480,"elapsed":665,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}},"outputId":"cbc0cc7d-3114-4632-de60-e7bd98b11854"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["The 0 th round is completed\n","The 9 th round is completed\n","The 18 th round is completed\n","The 27 th round is completed\n","The 36 th round is completed\n","The 45 th round is completed\n","The 54 th round is completed\n","The 63 th round is completed\n","The 72 th round is completed\n","The 81 th round is completed\n","The 90 th round is completed\n","The 99 th round is completed\n","its the final round (99 round), all finish\n"]}]},{"cell_type":"code","source":["# LEE\n","\n","import_pwd = '/content/drive/Shareddrives/Text Mining/CSV/Crawler/'\n","title_name = 'LEE_title_output.csv'\n","import_pwd_content = '/content/drive/Shareddrives/Text Mining/CSV/Clean_Content/'\n","clean_content_name = 'content_LEE.csv'\n","\n","LEE_df = title_content_merger(import_pwd,title_name,import_pwd_content,clean_content_name)\n","\n","# Save \n","save_path = \"/content/drive/Shareddrives/Text Mining/CSV/Summary/Stage1/\"\n","name = \"All_LEE.csv\"\n","path = save_path + name\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  LEE_df.to_csv(f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kE8L8SCVXWT","executionInfo":{"status":"ok","timestamp":1654587706136,"user_tz":-480,"elapsed":1225,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}},"outputId":"1f04b8a9-08e5-432f-d586-7ceb26ebe1bc"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["The 0 th round is completed\n","The 9 th round is completed\n","The 18 th round is completed\n","The 27 th round is completed\n","its the final round (32 round), all finish\n"]}]},{"cell_type":"code","source":["# YANG\n","\n","import_pwd = '/content/drive/Shareddrives/Text Mining/CSV/Crawler/'\n","title_name = 'YANG_title_output.csv'\n","import_pwd_content = '/content/drive/Shareddrives/Text Mining/CSV/Clean_Content/'\n","clean_content_name = 'content_YANG.csv'\n","\n","YANG_df = title_content_merger(import_pwd,title_name,import_pwd_content,clean_content_name)\n","\n","# Save \n","save_path = \"/content/drive/Shareddrives/Text Mining/CSV/Summary/Stage1/\"\n","name = \"All_YANG.csv\"\n","path = save_path + name\n","with open(path, 'w', encoding = 'utf-8-sig') as f:\n","  YANG_df.to_csv(f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"67MBtqC6VXYp","executionInfo":{"status":"ok","timestamp":1654587706779,"user_tz":-480,"elapsed":647,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}},"outputId":"da6bdcf5-c818-4a96-fc96-86fa035cfbef"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["The 0 th round is completed\n","The 9 th round is completed\n","The 18 th round is completed\n","The 27 th round is completed\n","The 36 th round is completed\n","The 45 th round is completed\n","The 54 th round is completed\n","The 63 th round is completed\n","The 72 th round is completed\n","The 81 th round is completed\n","The 90 th round is completed\n","The 99 th round is completed\n","its the final round (99 round), all finish\n"]}]},{"cell_type":"markdown","source":["## 進行句子壓縮"],"metadata":{"id":"f_VDwNM1Bbds"}},{"cell_type":"code","source":["# !pip install transformers==2.1.1\n","# !pip install sentencepiece\n","# !pip install tensorflow_addons\n","# !pip install hanziconv\n","\n","# import re\n","# from hanziconv import HanziConv\n","# import json\n","# import random"],"metadata":{"id":"qwSFKhERKvlw","executionInfo":{"status":"ok","timestamp":1654587706780,"user_tz":-480,"elapsed":5,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# import os\n","# os.chdir(\"/\")\n","\n","# from google.colab import drive\n","# try:\n","#     drive.mount('content/gdrive', force_remount=True)\n","# except:\n","#     print('你的 drive 掛載不成功')\n","    \n","# os.chdir(\"/content/gdrive/My Drive/\")"],"metadata":{"id":"Qj6bbDKz4GCE","executionInfo":{"status":"ok","timestamp":1654587706780,"user_tz":-480,"elapsed":4,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# files = []\n","# my_path_master = '/content/gdrive/MyDrive/GPT/GPT_chinese_trandition'\n","\n","# for r, d, f in os.walk(my_path_master):\n","#     for file in f:\n","        \n","#         files.append(os.path.join(r, file))\n","\n","# for f in files:\n","#     print(f)\n","\n","# os.chdir(my_path_master)"],"metadata":{"id":"UJdG98QY4GPI","executionInfo":{"status":"ok","timestamp":1654587706784,"user_tz":-480,"elapsed":8,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OX4rzzbt4GaW","executionInfo":{"status":"ok","timestamp":1654587706785,"user_tz":-480,"elapsed":9,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# # 1. 處理輸入資料\n","# def remove_punctuation(line):\n","\n","#     #rule = re.compile(\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n","\n","\n","#     line = line.replace(\"\\\\n\",\"\").replace(\"\\n\",\"\")\n","#     #\n","#     rule = re.compile(\"[\\[|\\]|\\\"|\\']\")\n","#     line = rule.sub('',line)\n","\n","#     #\n","#     rule = re.compile(\"[　| ]\")\n","#     line = rule.sub(',',line)\n","\n","#     #\n","#     line = HanziConv.toTraditional(line)\n","#     return line"],"metadata":{"id":"UI2uz357FhGt","executionInfo":{"status":"ok","timestamp":1654587706785,"user_tz":-480,"elapsed":9,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# output = []\n","\n","# wiki_zh_pwd = ''\n","# for r, d, f in os.walk(\"/content/gdrive/MyDrive/GPT/GPT_chinese_trandition/data/wiki_zh/\"):\n","#     for file in f:\n","#         if file.startswith('wiki'):\n","#             print(r+'/'+file)\n","#             with open(r+'/'+file,\"r\") as f:\n","#                 lines = f.readlines()\n","#                 for line in lines:\n","#                     sentence = json.loads(line)\n","#                     print(sentence.keys())\n","#                     templine = remove_punctuation(sentence['text'])\n","#                     print(templine)\n","#                     output.append(templine)\n","\n","# random.shuffle(output)\n","\n","# tran_json_pwd = \"./data/train.json\"\n","# with open(tran_json_pwd,\"w\") as f:\n","#     print(\"len\",len(output))\n","#     json.dump(output,f,ensure_ascii=False)\n","\n","# weight = 0.00\n","# for i in range(20):\n","#     weight+= 0.05\n","#     with open(\"./data/train_{}.json\".format(int(weight*100)),\"w\") as f:\n","#         count = int(len(output)*weight)\n","#         print(\"len\",count)\n","#         json.dump(output[:count],f,ensure_ascii=False)\n","\n"],"metadata":{"id":"maQvUNjnFkkp","executionInfo":{"status":"ok","timestamp":1654587706785,"user_tz":-480,"elapsed":8,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# # tokenize\n","# tokens_list = [ token_pack(article) for article in article_1]\n","\n","# clean_sentences = []\n","# def gs(tokens_list):\n","#   for article in tokens_list:\n","#     clean_tokens = []\n","#     for _ in article:\n","#       for token in _:\n","#         # 去除token所有半形空白\n","#         token = token.replace(' ', '')\n","#         # 前後去除空白 #去除中英夾雜 #去除純數字\n","#         if token.strip() != \"\" and not token.encode().isalpha()  and token.isdigit() == False:\n","#           # 去除自建停用字\n","#           if not token in full_stopwords:\n","#             # 去除英文半形標點符號  #去除中文之標點符號\n","#             if  not token in punctuation and not token in punctuation_str:\n","#               clean_tokens.append(token)\n","#       if len(clean_tokens)>0:\n","#         clean_sentences.append(clean_tokens)\n","#   return clean_sentences"],"metadata":{"id":"qWRpJbhmXbX2","executionInfo":{"status":"ok","timestamp":1654587706785,"user_tz":-480,"elapsed":8,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# new =  gs(tokens_list)"],"metadata":{"id":"Jew8qC2Bcruj","executionInfo":{"status":"ok","timestamp":1654587706785,"user_tz":-480,"elapsed":8,"user":{"displayName":"YS Shr","userId":"15499537188569097981"}}},"execution_count":26,"outputs":[]}]}